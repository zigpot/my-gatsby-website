"use strict";(self.webpackChunkmy_gatsby_site=self.webpackChunkmy_gatsby_site||[]).push([[227],{4314:function(e,t,n){var a=n(1151),r=n(7294);function o(e){const t=Object.assign({p:"p",em:"em",a:"a",pre:"pre",code:"code",img:"img",ol:"ol",li:"li"},(0,a.a)(),e.components);return r.createElement(r.Fragment,null,r.createElement(t.p,null,"I was recently obsessed with this funny video edit style: ",r.createElement(t.em,null,"the floating face effect"),". I made the name myself since I haven't found a specific name. It's basically a video effect where a face floats around in the video before it finally merge with it's original source. Let the video below explain:"),"\n",r.createElement(t.p,null,"https://twitter.com/babecabiita/status/1239880505824178176"),"\n",r.createElement(t.p,null,"Babe Cabiita is an Indonesian comedian, a funny guy he is."),"\n",r.createElement(t.p,null,"This results in a bizzare yet funny (and perhaps satisfying ü§î ?) effect. Below is another example of it."),"\n",r.createElement(t.p,null,"https://twitter.com/cunggun/status/1237772703404142592"),"\n",r.createElement(t.p,null,"This video is so viral, I've yet to find the original poster.T"),"\n",r.createElement(t.p,null,"At first I thought this was one of Tiktok's newest effect. However it wasn't. This effect was achieved in a rather manual way by it's creators, usually using Adobe Premiere and/or other dedicated video/image editing softwares. I'm not used with those kinds of software. Plus, since it's seems pretty easy, why not making it ourselves? After all, this seems like a simple challenge. We can fire up our Python editor. Also we are going to use ",r.createElement(t.a,{href:"https://opencv.org/"},"OpenCV"),"."),"\n",r.createElement(t.p,null,"For our example we are going to use this video from TikTok:"),"\n",r.createElement(t.p,null,"https://www.tiktok.com/@florence.margaret/video/6985066827550313755?is_copy_url=1&is_from_webapp=v1"),"\n",r.createElement(t.p,null,"Nice move, Flo!"),"\n",r.createElement(t.p,null,"We gonna have to detect a face. So, in this case we'll be using ",r.createElement(t.a,{href:"http://dlib.net/"},"dlib"),", a C++ library notorious for it's machine learning components. We are also going to use ",r.createElement(t.a,{href:"https://pypi.org/project/imutils/"},"imutils"),", for basic image manipulation. Also for matrix array manipulation, we'll ask our old friend ",r.createElement(t.a,{href:"https://numpy.org/"},"NumPy"),", a helping hand."),"\n",r.createElement(t.pre,null,r.createElement(t.code,null,"import cv2\nimport numpy as np\nfrom imutils import face_utils\nimport dlib\n\n# Videoplay\ncap = cv2.VideoCapture('video.mp4')\nif (cap.isOpened()== False): # Check if video opened successfully\n  print(\"Error opening video  file\")\n  quit()\n")),"\n",r.createElement(t.p,null,"First, we choose the frame of interest. And we assign the frame number of interest to a variable called ",r.createElement(t.em,null,"keyframe"),". That is the frame where you want to extract the face from, and later on 'merge' in the effect. Let's say, we choose this frame below for our experiment."),"\n",r.createElement(t.img,{src:"https://zigpot.files.wordpress.com/2021/09/frame298.png?w=540",alt:""}),"\n",r.createElement(t.p,null,"Frame 298 üòåüôè"),"\n",r.createElement(t.pre,null,r.createElement(t.code,null,"framecount = 0\nwhile(cap.isOpened()):\n  ret, frame = cap.read()\n  framecount = framecount + 1\n  cv2.imshow('Frame', frame)\n\n  if ret:\n    key = cv2.waitKey(25)\n    if key == -1:\n      continue\n    elif key == ord('q'):\n      quit()\n    elif key == ord('s'):\n      keyframe = framecount\n      cv2.imwrite('face_frame.png', frame)\n      print('Keyframe at', keyframe)\n      break\n  else:\n    break\ncv2.destroyAllWindows()\n")),"\n",r.createElement(t.p,null,"Seen above is the code snippet to play the video on a QT window, while it waits for the user to choose a specific frame (user presses 's'). However, if user presses 'q', video ends."),"\n",r.createElement(t.p,null,"Now that we have the frame of choice, we need to find a face, and extract it. For that, we'll be using an ",r.createElement(t.a,{href:"https://github.com/codeniko/shape_predictor_81_face_landmarks"},"81 points of facial landmarks detection model"),", as this one also includes the forehead (clue: most don't)."),"\n",r.createElement(t.pre,null,r.createElement(t.code,null,'#face extraction\n\ngray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\nout_face = np.zeros_like(frame)\n\ndetector = dlib.get_frontal_face_detector()\npredictor = dlib.shape_predictor("shape_predictor.dat")\n\nrects = detector(gray, 1)\n\nfor (i, rect) in enumerate(rects):\n   shape = predictor(gray, rect)\n   shape = face_utils.shape_to_np(shape)\n\n   remapped_shape = np.zeros_like(shape) \n   feature_mask = np.zeros((image.shape[0], image.shape[1]))   \n\n   remapped_shape = cv2.convexHull(shape)\n                            \n   cv2.fillConvexPoly(feature_mask, remapped_shape[0:27], 1)\n   feature_mask = feature_mask.astype(np.bool)\n   feature_mask = feature_mask * np.uint8(255)\n\n   new_img = cv2.cvtColor(image, cv2.COLOR_BGR2BGRA)\n   new_img[:, :, 3] = feature_mask\n   contours = cv2.findContours(feature_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n   contours = contours[0] if len(contours) == 2 else contours[1]\n   cntr = contours[0]\n   xpos,ypos,w,h = cv2.boundingRect(cntr)\n\n   face = new_img[ypos:ypos+h, xpos:xpos+w]\n\n   cv2.waitKey(0)\n   cv2.destroyAllWindows()\n\nface_height, face_width, _ = face.shape\n')),"\n",r.createElement(t.p,null,"The program above does the following:"),"\n",r.createElement(t.ol,null,"\n",r.createElement(t.li,null,"Detect face (called Region of Interest or ROI), this creates a feature mask."),"\n",r.createElement(t.li,null,"Blacken the area outside the ROI using the feature mask."),"\n",r.createElement(t.li,null,"Crop to the ROI."),"\n"),"\n",r.createElement(t.img,{src:"https://zigpot.files.wordpress.com/2021/09/faces-1.png?w=116",alt:""}),"\n",r.createElement(t.p,null,"Face cropped to it's surrounding."),"\n",r.createElement(t.p,null,"This crop region is square and doesn't look nice with the black areas around. We want a nice facial shape. To achieve this, we convert the face to a four channel data type (RGB + Alpha also known as the transparency). The feature mask is turned into a boolean matrix, it's black area gets zero-ed using a straightforward logic operation, and finally appended to the image as the alpha channel. In other words, the black area becomes transparent, while the white area is the only opaque area."),"\n",r.createElement(t.img,{src:"https://zigpot.files.wordpress.com/2021/09/out_facebottom-1.png?w=116",alt:""}),"\n",r.createElement(t.p,null,"Still square, but now it's transparent outside the ROI"),"\n",r.createElement(t.p,null,"Now that we have the face of interest, we want to put it somewhere on the video and move it until it reaches it's origin, where it was extracted from. With ",r.createElement(t.em,null,"origin")," I mean both the spatial (location) and temporal (frame) origin."),"\n",r.createElement(t.img,{src:"https://zigpot.files.wordpress.com/2021/09/float-1.png?w=540",alt:""}),"\n",r.createElement(t.p,null,"We want Flo's face to reach where it came from, on the exact source frame."),"\n",r.createElement(t.pre,null,r.createElement(t.code,null,"#Add face to video\n\ncap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n\nif(not cap.isOpened()):\n  print(\"Error opening video file\")\n  quit()\n\nframe_width = int(cap.get(3))\nprint(\"frame_width\",frame_width,\"xpos\", xpos)\nframe_height = int(cap.get(4))\n\nout = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc('M','J','P','G'), frame_rate, (frame_width,frame_height))\nframecount = 0\nkeyframe_relative = 50\nwhile(cap.isOpened()):\n  \n  ret, frame = cap.read()\n\n  x = int(xpos*framecount/102)\n  y = int(ypos*framecount/102)\n  if framecount <= 102:\n    x_ = x + face.shape[1]\n    y_ = y + face.shape[0]\n    alpha_face = face[:, :, 3]/255.0\n    alpha_frame = 1.0 - alpha_face\n    for c in range(0,3):\n      frame[y:y_, x:x_, c] = (alpha_face * face[:, :, c] + alpha_frame *frame[y:y_, x:x_, c])\n\n  if (ret):\n    framecount = framecount + 1\n    #cv2.imshow('Frame', frame)\n    out.write(frame)\n  else:\n    break\ncap.release()\nout.release()\n\ncv2.destroyAllWindows()\n")),"\n",r.createElement(t.p,null,"And here is the result!"),"\n",r.createElement(t.img,{src:"https://zigpot.files.wordpress.com/2021/09/output-1.gif?w=320",alt:""}),"\n",r.createElement(t.p,null,"Cool!"),"\n",r.createElement(t.p,null,"For the full code check out my GitHub project page (coming soon)!"),"\n",r.createElement(t.img,{src:"https://zigpot.files.wordpress.com/2021/09/kocak.gif?w=320",alt:""}))}t.Z=function(e){void 0===e&&(e={});const{wrapper:t}=Object.assign({},(0,a.a)(),e.components);return t?r.createElement(t,e,r.createElement(o,e)):o(e)}},2813:function(e,t,n){n.r(t),n.d(t,{Head:function(){return m},default:function(){return p}});var a=n(4314),r=n(7294),o=n(72),i=n(9357),l=n(2020),s=n(9089);const c=e=>{let{data:t,children:n}=e;return s.ZP.initialize("G-EGFPPG9G5L"),r.createElement(o.Z,{pageTitle:t.mdx.frontmatter.title},r.createElement("div",{className:"text-body"},r.createElement("div",{className:"text-author"},t.mdx.frontmatter.author),r.createElement("div",{className:"text-date"},(0,l.Z)(t.mdx.frontmatter.date)),n))},m=e=>{let{data:t}=e;return r.createElement(i.Z,{title:t.mdx.frontmatter.title})};function p(e){return r.createElement(c,e,r.createElement(a.Z,e))}}}]);
//# sourceMappingURL=component---src-pages-literatures-mdx-frontmatter-slug-js-content-file-path-blog-hello-and-making-floating-face-effect-index-mdx-e3227da74500ce4eafca.js.map